{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using MPS\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './train_data/wkz8.txt'\n",
    "ctx_len = 64\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 87542\n",
      "Number of unique characters in the file: 2149\n",
      "Unique characters: \n",
      " *.08altwx—‘’“”…、。一丁七万丈三上下不与丑专且世丘业东丝丢两严个丫中丸丹为主丽举乃久么义之乌乍乎乐乔乖乘九也书买乱了予争事二于亏云互五井些亡交亮亲人亿什仇今仍从仑仔他仗仙仞代令以仨仪们仰件价任份仿伍伏伐休众优伙会伟传伤伦伶伸似但位低住佑体何余佛作你佩佼使依侠侧便促俊俏俗保信俩修俯俱俺倒候倚借倦值倾假偈偎偏做停偷偿傲傻像僧儿兀元兄充先光克免兔入全八公六兮兰共关兴兵其具养兽内冉再冒冕写冠冤冥冬冰冲决况冷冻净凄准凉凌减凑凛凝几凡凤凭凰凳凶出击刀分切刑划刚初判利别刮到制刹刺刻前剑剥剧剩副割劈力劝办功加务劣动助努劫劲劳势勇勒勾包匆化北匹区医十千升午半华卑卒单卖南博占卫卯印危即却卷历厉压厌厚原厮去参又及友双反发取受变叛叠口古句另只叫叭叮可台史右叶号叹叽吁吃各吆合同名后吏吐向吓吗君吞吟否吧吩含听吭启吵吸吹吻吼吽呀呆告呜呢呦周味呵呸呼命咆咋和咏咐咒咔咕咚咦咧咪咬咱咳咽哀品哄哇哈哉响哎哑哗哟哥哦哧哩哪哭哮哲哼哽唉唐唠唤唬唯唱唳唵啃啄啊啥啦啪啷啸啼喂喃善喊喘喜喝喧喳喷喽嗔嗝嗡嗨嗬嗯嗵嗷嘎嘘嘛嘟嘤嘲嘴嘶嘻嘿噙噢器嚎嚓嚣嚼囚四回因团园困围固国图圆圈土圣在地场均坏坐坑块坚坠坡垂型垒垫埋城堂堆堵塌塔塞填境墓墙增墟壁壑壤士声壳壶处备复夏夕外多夜够大天太夫央失头夹夺奇奈奋奔套奘奥女奶她好如妃妄妇妈妖妙妨妹始姐姑姓姿威娘娲婉婢婶媳嫁嫩子孔字存孙季孤学孩孱宁它宇守安完宗官宙定宛宜宝实客宫宰害宴家容宽宿寂密寒寞察寸对寺寻导封射将尊小少尖尘尚尝尤就尸尺尽尾局屁层居屈屋屏屑展山岁岂岐岔岚岛岩岭岸峰峻崖崩崽嶽巉巡左巧巨差己已巴巾市布帅师希帘帝带席帮常帽幅幕干平年并幸幻幽广庄庆床序应底庙府庞废度座庭廊廖延建开异弃弄式引弟张弥弧弯弱弹强弼归当形彩影彼往径待很律徐徒得御微德心必忆忌忍忒志忘忙忧快念忽怀态怎怏怒怔怕怖怜思急性怨怪怯总恋恍恐恒恙恨恩恬恭息恳恶恼悄悉悔悟悠悦您悬悲情惊惑惜惧惨惬惮想惶惹愁愉意愕感愣愤愿慌慕慢慧憧憬憾懂懒戏成我戒或战戟戳戴所扁扇手才扎扑打扔托执扫扬扮扯扰扶找承把抓投抖抗折抚抛抢护报披抬抱抵抹押抽拂拄担拆拉拍拎拐拒拖拘招拜拣拥拦拨择拯拱拳拼拽拾拿持挂指按挑挖挚挠挡挣挤挥挨振挺挽捂捅捉捏捕捞捡换捣捧据捶捷掀掂授掉掌掏掐排掠探接推掩措掰揉揍提插握揣揪揭揽搂搅搐搜搞搡搬搭摄摆摇摈摊摔摘摸撇撑撒撕撞撬擒擞攀攥支收改攻放故敌救敖教敢散敬数敲整文斗料斧斩断斯新方施旁旅旋族无既日旦旧旨早时旷昂昆昌明昏易星映春昨是显晃晌晕晚晨景晰晶智晾暂暇暖暗暴曜曰曲更曾替最月有朋服朔朗望朝期朦木未末本术朵机朽杀杂杆杈李村杖束条来杨杯松极枉枕林果枝枪枯架柏某染柔查柯柱柳柴标栏树栗株样核根格栽桂桃桌桑桔桥桩桶梅梢梦检棋棍棒森棵椅植椰楚楼概槌模横欠次欢欣欲欺歇歌止正此步武歪歹死殃残段殿毁母每比毕毛毫毯民氓气水永汁求汇汉汗江汪汹汾沉沙没河油沽沾沿泄泉泊泌法泞泡波泣泥注泪泳泼洁洋洒洗洞洪洲活派流测浑浓浙浩浪浮海浸涂消涉涌涡润涧涨涯液涵淇淋淌淘淡深混淹清渊渐渗渡渣温渴游渺湖湿溃溅源溜溶滋滑滔滚满滴漂漉漏漓演漠漫漾潜潭潮澈澡激瀑瀛火灭灯灰灵灾灿炉炎炭炸点炼烁烂烈烟烦烧烬热焦焰然照熄熊熔熟燃爆爪爬爱父爷爸爹爽片牙牛牢物牵特犀犯状犹狂狗狞狠独狮狰狱狲狼猎猕猛猜猡猢猪猫猴猿玄玉王玩环现玷珍珏珠球理琉琐琵琶瑞瑟瑶璃瓜瓢瓤瓦甘甚生用甩田由甲电男画畅界畏畔留畜疑疯疼疾病症痒痕痛痴瘫登白百的皆皇皈皋皮皱盂盆盈盏监盔盖盘盛目盯直相盼省眉看真眨眩眼着睁睛睡睬瞄瞌瞟瞧瞪瞬瞻矗矣知矩短矮石砂砌砍研砖砰破砸硬确碍碎碗碧碰磕磨礼祖祗祝神祥祸禀禁禅福离秀私秃秋种秘积称移稀程稚稳稽穆究穷穹空穿突窃窍窗窜窝立竖站竞竟章童竭端竹笑笠笨第笼等筋筏筐筑答简箍算管箭篁篇篮篷簿籍米类粉粒粗精糊糟系索紧紫累红纤约级纪纯纱纳纵纷纸纹线练组细织终经结绕给绚绝继绪续维绸绽绿缁缓缕编缘缚缠缩缸缺罐网罕罗罚罢罩罪置美羞羡群羽翅翎翔翘翠翻翼耀老考者而耍耐耙耳耶耸聊联聚聪肃肆肉肋肌肚肠股肢肥肩肯肿胆背胎胖胜胡胧胸能脆脉脏脑脖脚脱脸腐腑腔腕腥腰腹腻腾腿膀膝臂臣自臭至舌舍舒舔舞般船艘良艰色艳艺节芒芝芦花苍苏苔苗若苦英茫茶草荒荡荣荫药莫莲莹菩萎萤萦萧萨落葫葬葱葵蒙蒸蓝蓬蔓蔽薄薪薯藏藓藤虎虑虚虫虹虽蚁蚂蛋蜂蜓蜗蜻蝇蝉蝗蝴蝶融蟠蠕蠢血行衍衔街衣补表衫衰衲袋袍袖被袭袱裂装裙裳裸裹襟西要覆见观规视览觉角解触言誉誓计订认讨让议记讲讶许论设访诀证诅识诈诉词试诗诚话该诬语误诱说诵诶请诸读谁调谈谊谋谎谒谛谢谨谪谷象豹貌贝负责败账质贪贱贴贵费贺资赌赎赏赐赔赖赚赛赞赢赤赦赫走赶起超越趟趣足趴跃跄跋跌跑跟跪路跳踉踏踢踩踪踱蹈蹑蹦蹲身躯躲躺车转轮软轰轻载辈辉输辗辛辞辟辰辱边达迅过迈迎运近返还这进远连迟迫迳迷迸迹追退送适逃逆选逊逍透逐递途逗通逝逞速造逢逼遁遇遍遐道遣遥遮避邀那郁郎部都配酒酥酷酸酿醇醉醒采释里重野量金钉钟钢钧钱钵钻铁铛铜铭银铸铺链销锁锅锋错锦键镇镜镶长门闪闭问闯闲间闷闸闹闻阎防阳阴阵阶阻阿陀附际陆陌降限院除险陪陶陷隆随隐隔难雀雁雄集雕雨雪雳零雷雾需霄震霉霜霞露霹青静非靠面靴鞋音顶顺须顽顾顿颂预颅领颇颈颊颗题颜额颠颤风飓飘飞食饥饭饮饰饵饶饿首香马驮驯驳驶骂骄骑骗骨骸高鬼魂魄魔鱼鲜鲤鲨鳅鸟鸡鸣鸭鸿鹏鹤鹰鹿麻黄黎黑默黛黝黯鼓鼠鼻齐齿龙！＃％，：；？～\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Count unique characters\n",
    "unique_chars = set(text)\n",
    "num_unique_chars = len(unique_chars)\n",
    "\n",
    "print(f'Length of text: {len(text)}')\n",
    "print(f\"Number of unique characters in the file: {num_unique_chars}\")\n",
    "print(\"Unique characters:\", ''.join(sorted(unique_chars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[756]\n",
      "['你']\n"
     ]
    }
   ],
   "source": [
    "character_to_index = {char: i for i, char in enumerate(unique_chars)}\n",
    "index_to_character = {i: char for i, char in enumerate(unique_chars)}\n",
    "encode = lambda x: [character_to_index[i] for i in x]\n",
    "decode = lambda x: [index_to_character[i] for i in x]\n",
    "\n",
    "print(encode('你'))\n",
    "print(decode(encode('你')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Precompute sinusoidal embeddings\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer(\"sinusoidal\", torch.einsum(\"i,j->ij\", torch.arange(max_seq_len).float(), inv_freq))\n",
    "        self.register_buffer(\"sin\", torch.sin(self.sinusoidal))\n",
    "        self.register_buffer(\"cos\", torch.cos(self.sinusoidal))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: A tensor of shape (length, batch, d_model).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (length, batch, d_model) with rotary positional embeddings applied.\n",
    "        \"\"\"\n",
    "        length, batch, d_model = x.shape\n",
    "        assert d_model == self.d_model, \"Input d_model must match initialized d_model\"\n",
    "\n",
    "        # Apply rotary embeddings\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split into even and odd dimensions\n",
    "        x_rotated = torch.cat([x1 * self.cos[:length, None, :] - x2 * self.sin[:length, None, :],\n",
    "                               x1 * self.sin[:length, None, :] + x2 * self.cos[:length, None, :]], dim=-1)\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ctx_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.rope = RotaryPositionalEmbedding(d_model, max_seq_len = ctx_len)\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # rope only applies to q and k, not v\n",
    "        q = self.wq(x)\n",
    "        q = self.rope(q)\n",
    "        k = self.wk(x)\n",
    "        k = self.rope(k)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_heads, self.head_dim)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_heads, self.head_dim)\n",
    "        v = v.view(v.shape[0], v.shape[1], self.n_heads, self.head_dim)\n",
    "\n",
    "        # Assume the input is of shape (length, batch, d_model)\n",
    "        # the Q, K, V tensors are now of shape (length, batch, n_heads, head_dim)\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "        # now they are of shape (batch, n_heads, length, head_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # the operations so far can be done with einsum in a much more succinct way i suppose\n",
    "        out = attn @ v\n",
    "        out = out.permute(2, 0, 1, 3).reshape(x.shape[0], x.shape[1], self.d_model)\n",
    "        out = self.wo(out)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model, ctx_len, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.ctx_len = ctx_len\n",
    "        self.embedding = nn.Embedding(vocab, d_model)\n",
    "        self.mha = nn.ModuleList([MHA(d_model, n_heads, ctx_len) for i in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.mha:\n",
    "            x = layer(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "GPT                                                --\n",
       "├─Embedding: 1-1                                   1,100,288\n",
       "├─ModuleList: 1-2                                  --\n",
       "│    └─MHA: 2-1                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-1         --\n",
       "│    │    └─Linear: 3-2                            262,656\n",
       "│    │    └─Linear: 3-3                            262,656\n",
       "│    │    └─Linear: 3-4                            262,656\n",
       "│    │    └─Linear: 3-5                            262,656\n",
       "│    │    └─Sequential: 3-6                        2,099,712\n",
       "│    └─MHA: 2-2                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-7         --\n",
       "│    │    └─Linear: 3-8                            262,656\n",
       "│    │    └─Linear: 3-9                            262,656\n",
       "│    │    └─Linear: 3-10                           262,656\n",
       "│    │    └─Linear: 3-11                           262,656\n",
       "│    │    └─Sequential: 3-12                       2,099,712\n",
       "│    └─MHA: 2-3                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-13        --\n",
       "│    │    └─Linear: 3-14                           262,656\n",
       "│    │    └─Linear: 3-15                           262,656\n",
       "│    │    └─Linear: 3-16                           262,656\n",
       "│    │    └─Linear: 3-17                           262,656\n",
       "│    │    └─Sequential: 3-18                       2,099,712\n",
       "│    └─MHA: 2-4                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-19        --\n",
       "│    │    └─Linear: 3-20                           262,656\n",
       "│    │    └─Linear: 3-21                           262,656\n",
       "│    │    └─Linear: 3-22                           262,656\n",
       "│    │    └─Linear: 3-23                           262,656\n",
       "│    │    └─Sequential: 3-24                       2,099,712\n",
       "│    └─MHA: 2-5                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-25        --\n",
       "│    │    └─Linear: 3-26                           262,656\n",
       "│    │    └─Linear: 3-27                           262,656\n",
       "│    │    └─Linear: 3-28                           262,656\n",
       "│    │    └─Linear: 3-29                           262,656\n",
       "│    │    └─Sequential: 3-30                       2,099,712\n",
       "│    └─MHA: 2-6                                    --\n",
       "│    │    └─RotaryPositionalEmbedding: 3-31        --\n",
       "│    │    └─Linear: 3-32                           262,656\n",
       "│    │    └─Linear: 3-33                           262,656\n",
       "│    │    └─Linear: 3-34                           262,656\n",
       "│    │    └─Linear: 3-35                           262,656\n",
       "│    │    └─Sequential: 3-36                       2,099,712\n",
       "├─Linear: 1-3                                      1,102,437\n",
       "===========================================================================\n",
       "Total params: 21,104,741\n",
       "Trainable params: 21,104,741\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GPT(vocab=num_unique_chars,\n",
    "            d_model=d_model,\n",
    "            ctx_len=ctx_len,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2149])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, num_unique_chars, (ctx_len,1))\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
