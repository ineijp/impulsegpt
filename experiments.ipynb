{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(\"Using CUDA\")\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"Using MPS\")\n",
    "else:\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './train_data/wkz8.txt'\n",
    "ctx_len = 64\n",
    "batch_size = 8\n",
    "d_model = 512\n",
    "n_heads = 8\n",
    "n_layers = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the text file\n",
    "with open(data_dir, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Count unique characters\n",
    "unique_chars = set(text)\n",
    "num_unique_chars = len(unique_chars)\n",
    "\n",
    "print(f'Length of text: {len(text)}')\n",
    "print(f\"Number of unique characters in the file: {num_unique_chars}\")\n",
    "print(\"Unique characters:\", ''.join(sorted(unique_chars)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_to_index = {char: i for i, char in enumerate(unique_chars)}\n",
    "index_to_character = {i: char for i, char in enumerate(unique_chars)}\n",
    "encode = lambda x: [character_to_index[i] for i in x]\n",
    "decode = lambda x: [index_to_character[i] for i in x]\n",
    "\n",
    "print(encode('你'))\n",
    "print(decode(encode('你')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len):\n",
    "        super(RotaryPositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        # Precompute sinusoidal embeddings\n",
    "        inv_freq = 1.0 / (10000 ** (torch.arange(0, d_model, 2).float() / d_model))\n",
    "        self.register_buffer(\"sinusoidal\", torch.einsum(\"i,j->ij\", torch.arange(max_seq_len).float(), inv_freq))\n",
    "        self.register_buffer(\"sin\", torch.sin(self.sinusoidal))\n",
    "        self.register_buffer(\"cos\", torch.cos(self.sinusoidal))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: A tensor of shape (length, batch, d_model).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (length, batch, d_model) with rotary positional embeddings applied.\n",
    "        \"\"\"\n",
    "        length, batch, d_model = x.shape\n",
    "        assert d_model == self.d_model, \"Input d_model must match initialized d_model\"\n",
    "\n",
    "        # Apply rotary embeddings\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]  # Split into even and odd dimensions\n",
    "        x_rotated = torch.cat([x1 * self.cos[:length, None, :] - x2 * self.sin[:length, None, :],\n",
    "                               x1 * self.sin[:length, None, :] + x2 * self.cos[:length, None, :]], dim=-1)\n",
    "        return x_rotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHA(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, ctx_len):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.rope = RotaryPositionalEmbedding(d_model, max_seq_len = ctx_len)\n",
    "        self.wq = nn.Linear(d_model, d_model)\n",
    "        self.wk = nn.Linear(d_model, d_model)\n",
    "        self.wv = nn.Linear(d_model, d_model)\n",
    "        self.wo = nn.Linear(d_model, d_model)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, 4 * d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * d_model, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # rope only applies to q and k, not v\n",
    "        q = self.wq(x)\n",
    "        q = self.rope(q)\n",
    "        k = self.wk(x)\n",
    "        k = self.rope(k)\n",
    "        v = self.wv(x)\n",
    "\n",
    "        q = q.view(q.shape[0], q.shape[1], self.n_heads, self.head_dim)\n",
    "        k = k.view(k.shape[0], k.shape[1], self.n_heads, self.head_dim)\n",
    "        v = v.view(v.shape[0], v.shape[1], self.n_heads, self.head_dim)\n",
    "\n",
    "        # Assume the input is of shape (length, batch, d_model)\n",
    "        # the Q, K, V tensors are now of shape (length, batch, n_heads, head_dim)\n",
    "        q = q.permute(1, 2, 0, 3)\n",
    "        k = k.permute(1, 2, 0, 3)\n",
    "        v = v.permute(1, 2, 0, 3)\n",
    "        # now they are of shape (batch, n_heads, length, head_dim)\n",
    "        attn = (q @ k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        # the operations so far can be done with einsum in a much more succinct way i suppose\n",
    "        out = attn @ v\n",
    "        out = out.permute(2, 0, 1, 3).reshape(x.shape[0], x.shape[1], self.d_model)\n",
    "        out = self.wo(out)\n",
    "        out = self.ff(out)\n",
    "        return out\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab, d_model, ctx_len, n_heads, n_layers):\n",
    "        super().__init__()\n",
    "        self.vocab = vocab\n",
    "        self.d_model = d_model\n",
    "        self.ctx_len = ctx_len\n",
    "        self.embedding = nn.Embedding(vocab, d_model)\n",
    "        self.mha = nn.ModuleList([MHA(d_model, n_heads, ctx_len) for i in range(n_layers)])\n",
    "        self.fc = nn.Linear(d_model, vocab)\n",
    " \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        for layer in self.mha:\n",
    "            x = layer(x)\n",
    "        x = self.fc(x[-1])\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT(vocab=num_unique_chars,\n",
    "            d_model=d_model,\n",
    "            ctx_len=ctx_len,\n",
    "            n_heads=n_heads,\n",
    "            n_layers=n_layers)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2149])\n",
      "tensor([[-0.0296, -0.0054,  0.0118,  ...,  0.0462, -0.0051,  0.0271]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "tensor(1188)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randint(1, num_unique_chars, (ctx_len,1))\n",
    "y = model(x)\n",
    "print(y.shape)\n",
    "print(y)\n",
    "y = F.softmax(y, dim=-1)\n",
    "print(torch.argmax(y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
